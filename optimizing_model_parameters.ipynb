{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "789644a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=\"False\",\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data,batch_size=64)\n",
    "test_dataloader = DataLoader(training_data,batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "718f6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a64b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8fe0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2180e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,model,loss_fn,optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0 :\n",
    "            loss,current = loss.item(),batch*batch_size + len(X)\n",
    "            print(loss,current,size)\n",
    "\n",
    "def test_loop(data_loader,model,loss_fn):\n",
    "    model.eval()\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    test_loss,correct = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_loader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(100*correct,test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a01f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "2.2993292808532715 64 60000\n",
      "2.292097330093384 6464 60000\n",
      "2.2710788249969482 12864 60000\n",
      "2.2636895179748535 19264 60000\n",
      "2.25504732131958 25664 60000\n",
      "2.2218029499053955 32064 60000\n",
      "2.2274434566497803 38464 60000\n",
      "2.1868531703948975 44864 60000\n",
      "2.1874310970306396 51264 60000\n",
      "2.1600677967071533 57664 60000\n",
      "45.50333333333334 2.151821105210766\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "2.1618759632110596 64 60000\n",
      "2.152878761291504 6464 60000\n",
      "2.0928914546966553 12864 60000\n",
      "2.1105809211730957 19264 60000\n",
      "2.06526780128479 25664 60000\n",
      "2.000793695449829 32064 60000\n",
      "2.0263566970825195 38464 60000\n",
      "1.9403131008148193 44864 60000\n",
      "1.9464349746704102 51264 60000\n",
      "1.8791674375534058 57664 60000\n",
      "63.79666666666667 1.8690315027480948\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "1.9079623222351074 64 60000\n",
      "1.870216965675354 6464 60000\n",
      "1.7549924850463867 12864 60000\n",
      "1.7996983528137207 19264 60000\n",
      "1.6944106817245483 25664 60000\n",
      "1.6455374956130981 32064 60000\n",
      "1.667769193649292 38464 60000\n",
      "1.566826343536377 44864 60000\n",
      "1.5898394584655762 51264 60000\n",
      "1.4897700548171997 57664 60000\n",
      "64.44 1.4972625885690962\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "1.5745680332183838 64 60000\n",
      "1.5298763513565063 6464 60000\n",
      "1.3888275623321533 12864 60000\n",
      "1.4570918083190918 19264 60000\n",
      "1.3436589241027832 25664 60000\n",
      "1.3382418155670166 32064 60000\n",
      "1.3533533811569214 38464 60000\n",
      "1.2779488563537598 44864 60000\n",
      "1.310657262802124 51264 60000\n",
      "1.2199922800064087 57664 60000\n",
      "65.24666666666667 1.2309724856287176\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "1.3196529150009155 64 60000\n",
      "1.2907252311706543 6464 60000\n",
      "1.1369380950927734 12864 60000\n",
      "1.2364596128463745 19264 60000\n",
      "1.1143958568572998 25664 60000\n",
      "1.137648582458496 32064 60000\n",
      "1.1616997718811035 38464 60000\n",
      "1.0991307497024536 44864 60000\n",
      "1.137528896331787 51264 60000\n",
      "1.0655324459075928 57664 60000\n",
      "66.185 1.0667250169746911\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "1.1509431600570679 64 60000\n",
      "1.1423581838607788 6464 60000\n",
      "0.9718493819236755 12864 60000\n",
      "1.1002864837646484 19264 60000\n",
      "0.9739658832550049 25664 60000\n",
      "1.0061862468719482 32064 60000\n",
      "1.0455065965652466 38464 60000\n",
      "0.988098680973053 44864 60000\n",
      "1.0258984565734863 51264 60000\n",
      "0.9703474640846252 57664 60000\n",
      "67.26666666666667 0.9614757187585078\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "1.0350815057754517 64 60000\n",
      "1.0476875305175781 6464 60000\n",
      "0.8594866991043091 12864 60000\n",
      "1.0099611282348633 19264 60000\n",
      "0.8855854272842407 25664 60000\n",
      "0.9151853919029236 32064 60000\n",
      "0.9703449606895447 38464 60000\n",
      "0.9168248772621155 44864 60000\n",
      "0.9488942623138428 51264 60000\n",
      "0.9065268039703369 57664 60000\n",
      "68.455 0.8897166550159454\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "0.9504616260528564 64 60000\n",
      "0.982528805732727 6464 60000\n",
      "0.7790504693984985 12864 60000\n",
      "0.9456246495246887 19264 60000\n",
      "0.8263857960700989 25664 60000\n",
      "0.8487381935119629 32064 60000\n",
      "0.9174257516860962 38464 60000\n",
      "0.869231641292572 44864 60000\n",
      "0.8931418061256409 51264 60000\n",
      "0.8600990176200867 57664 60000\n",
      "69.59166666666667 0.8377544221593373\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "0.8851328492164612 64 60000\n",
      "0.9338614344596863 6464 60000\n",
      "0.7186779379844666 12864 60000\n",
      "0.8972790241241455 19264 60000\n",
      "0.7841824293136597 25664 60000\n",
      "0.798568844795227 32064 60000\n",
      "0.8770513534545898 38464 60000\n",
      "0.8358326554298401 44864 60000\n",
      "0.8511008024215698 51264 60000\n",
      "0.8239640593528748 57664 60000\n",
      "70.81666666666668 0.7980263486726961\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "0.8323419094085693 64 60000\n",
      "0.8947216272354126 6464 60000\n",
      "0.671169102191925 12864 60000\n",
      "0.8595014214515686 19264 60000\n",
      "0.7520599961280823 25664 60000\n",
      "0.759679913520813 32064 60000\n",
      "0.8440418839454651 38464 60000\n",
      "0.8109714984893799 44864 60000\n",
      "0.8180942535400391 51264 60000\n",
      "0.7944161295890808 57664 60000\n",
      "72.17 0.7661489229212438\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
